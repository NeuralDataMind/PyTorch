{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOo32Z3VWVTSa9RYSpTiESJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NeuralDataMind/PyTorch/blob/main/Pre_trained_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swKFd9eWv1TQ",
        "outputId": "46ea0d4d-8c0e-4249-a8c9-6e3259a0df52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "# ðŸ”§ Step 1: Install and Import\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DIR = r'/content/synthetic_stock_patterns.csv'"
      ],
      "metadata": {
        "id": "dFLqXK2nv8dq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d66d1524"
      },
      "source": [
        "# Task\n",
        "Explain the error in the provided code for a CNN-LSTM model for stock market prediction, fix it if possible, and incorporate the changes. If not, diagnose the error. The model is overfitting, with accuracy dropping to 16%. The task is to create a PyTorch script for a CNN-LSTM pretraining model for predicting stock market movement (up or down) using the data from \"synthetic_stock_patterns.csv\". The variable `DIR` holds the path to this file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c798601"
      },
      "source": [
        "## Define the cnn-lstm model\n",
        "\n",
        "### Subtask:\n",
        "Implement the CNN-LSTM model architecture in PyTorch, combining convolutional layers for feature extraction from the time series data and LSTM layers to capture sequential dependencies. The output layer should be designed for binary classification (up or down).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd66bdb9"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the CNN-LSTM model architecture in PyTorch as instructed, including convolutional layers, pooling, an LSTM layer, and a final fully connected layer for binary classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ec8a90c"
      },
      "source": [
        "class CNN_LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes, sequence_length):\n",
        "        super(CNN_LSTM, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.sequence_length = sequence_length\n",
        "\n",
        "        # CNN layers\n",
        "        self.conv1d = nn.Conv1d(input_size, 64, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool1d = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calculate the output size of CNN to determine input size for LSTM\n",
        "        # The output size after conv1d with padding='same' (or padding=1 with kernel_size=3) is the same as input size\n",
        "        # After maxpool1d with kernel_size=2 and stride=2, the size is halved\n",
        "        cnn_output_size = 64  # number of filters in conv1d\n",
        "        pooled_sequence_length = self.sequence_length // 2 # This calculation needs to be exact based on input dimensions\n",
        "\n",
        "        self.lstm = nn.LSTM(cnn_output_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size * pooled_sequence_length, num_classes) # LSTM output is (batch_size, seq_len, hidden_size), need to flatten\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, sequence_length, input_size)\n",
        "        # Permute to (batch_size, input_size, sequence_length) for Conv1d\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        # CNN forward pass\n",
        "        out = self.conv1d(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool1d(out)\n",
        "\n",
        "        # Reshape for LSTM layer: (batch_size, pooled_sequence_length, cnn_output_size)\n",
        "        out = out.permute(0, 2, 1)\n",
        "        batch_size = out.size(0)\n",
        "\n",
        "        # LSTM forward pass\n",
        "        # LSTM input shape: (batch_size, sequence_length, input_size)\n",
        "        # LSTM output shape: (batch_size, sequence_length, hidden_size)\n",
        "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
        "\n",
        "        out, _ = self.lstm(out, (h0, c0))\n",
        "\n",
        "        # Flatten the output for the fully connected layer\n",
        "        out = out.reshape(batch_size, -1)\n",
        "\n",
        "        # Fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1b1bdce7"
      },
      "source": [
        "## Prepare the data\n",
        "\n",
        "### Subtask:\n",
        "Ensure the data is in the correct format for the CNN-LSTM model. This might involve reshaping the data to have the channels dimension in the correct place for the CNN layers and ensuring the time series data is suitable for the LSTM.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "957b9448"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the data, separate features and target, convert to numpy arrays, reshape features, convert target to numerical labels, split data, and convert to PyTorch tensors.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e79d5bf2",
        "outputId": "7a30da0f-a20f-44ec-90fa-f986648a0167"
      },
      "source": [
        "# Step 1: Load the data\n",
        "df = pd.read_csv(DIR)\n",
        "\n",
        "# Step 2: Separate features and target\n",
        "features = df.drop('label', axis=1)\n",
        "target = df['label']\n",
        "\n",
        "# Step 3: Convert to NumPy arrays\n",
        "features_np = features.values\n",
        "target_np = target.values\n",
        "\n",
        "# Step 4: Reshape features for CNN-LSTM\n",
        "# Assuming the original features are structured as (num_samples, window_size * num_features_per_day)\n",
        "# We need to reshape to (num_samples, sequence_length, num_features_per_day)\n",
        "# In this dataset, window_size is 60 and num_features_per_day is 5 (open, high, low, close, volume)\n",
        "sequence_length = 60\n",
        "num_features_per_day = 5\n",
        "num_samples = features_np.shape[0]\n",
        "features_reshaped = features_np.reshape(num_samples, sequence_length, num_features_per_day)\n",
        "\n",
        "# Step 5: Convert target to numerical format\n",
        "label_mapping = {label: idx for idx, label in enumerate(np.unique(target_np))}\n",
        "target_numerical = np.array([label_mapping[label] for label in target_np])\n",
        "num_classes = len(label_mapping)\n",
        "\n",
        "# Step 6: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(features_reshaped, target_numerical, test_size=0.2, random_state=42, stratify=target_numerical)\n",
        "\n",
        "# Step 7: Convert to PyTorch tensors and move to device\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "\n",
        "print(\"Data successfully processed and converted to tensors.\")\n",
        "print(\"X_train_tensor shape:\", X_train_tensor.shape)\n",
        "print(\"y_train_tensor shape:\", y_train_tensor.shape)\n",
        "print(\"X_test_tensor shape:\", X_test_tensor.shape)\n",
        "print(\"y_test_tensor shape:\", y_test_tensor.shape)\n",
        "print(\"Number of classes:\", num_classes)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data successfully processed and converted to tensors.\n",
            "X_train_tensor shape: torch.Size([2800, 60, 5])\n",
            "y_train_tensor shape: torch.Size([2800])\n",
            "X_test_tensor shape: torch.Size([700, 60, 5])\n",
            "y_test_tensor shape: torch.Size([700])\n",
            "Number of classes: 7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4547bf78"
      },
      "source": [
        "## Training loop\n",
        "\n",
        "### Subtask:\n",
        "Implement the training loop, including defining the loss function and optimizer, and iterating through the data in batches to train the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ee48856"
      },
      "source": [
        "**Reasoning**:\n",
        "Implement the training loop for the CNN-LSTM model, including defining the loss function, optimizer, and iterating through batches for training and evaluating on the test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24c53d79",
        "outputId": "274f183b-95db-4547-d2e0-e8d91b8c6578"
      },
      "source": [
        "# Instantiate the model\n",
        "input_size = num_features_per_day\n",
        "hidden_size = 128 # Example hidden size\n",
        "num_layers = 2 # Example number of layers\n",
        "model = CNN_LSTM(input_size, hidden_size, num_layers, num_classes, sequence_length).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) # Example batch size\n",
        "\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Example batch size\n",
        "\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10 # Example number of epochs\n",
        "best_accuracy = 0.0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Print training loss per epoch\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "    # Evaluation on the test set\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad(): # Disable gradient calculation\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        print(f'Accuracy of the model on the {total} test samples: {accuracy:.2f} %')\n",
        "\n",
        "        # Save the best model\n",
        "        if accuracy > best_accuracy:\n",
        "            best_accuracy = accuracy\n",
        "            torch.save(model.state_dict(), 'best_cnn_lstm_model.pth')\n",
        "            print(f'Saved best model with accuracy: {best_accuracy:.2f} %')\n",
        "\n",
        "print(\"Finished Training\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 1.9499\n",
            "Accuracy of the model on the 700 test samples: 13.00 %\n",
            "Saved best model with accuracy: 13.00 %\n",
            "Epoch [2/10], Loss: 1.9447\n",
            "Accuracy of the model on the 700 test samples: 14.29 %\n",
            "Saved best model with accuracy: 14.29 %\n",
            "Epoch [3/10], Loss: 1.9388\n",
            "Accuracy of the model on the 700 test samples: 15.43 %\n",
            "Saved best model with accuracy: 15.43 %\n",
            "Epoch [4/10], Loss: 1.9262\n",
            "Accuracy of the model on the 700 test samples: 19.14 %\n",
            "Saved best model with accuracy: 19.14 %\n",
            "Epoch [5/10], Loss: 1.8693\n",
            "Accuracy of the model on the 700 test samples: 26.57 %\n",
            "Saved best model with accuracy: 26.57 %\n",
            "Epoch [6/10], Loss: 1.6803\n",
            "Accuracy of the model on the 700 test samples: 38.57 %\n",
            "Saved best model with accuracy: 38.57 %\n",
            "Epoch [7/10], Loss: 1.4974\n",
            "Accuracy of the model on the 700 test samples: 42.14 %\n",
            "Saved best model with accuracy: 42.14 %\n",
            "Epoch [8/10], Loss: 1.3518\n",
            "Accuracy of the model on the 700 test samples: 48.57 %\n",
            "Saved best model with accuracy: 48.57 %\n",
            "Epoch [9/10], Loss: 1.3091\n",
            "Accuracy of the model on the 700 test samples: 48.14 %\n",
            "Epoch [10/10], Loss: 1.2000\n",
            "Accuracy of the model on the 700 test samples: 51.86 %\n",
            "Saved best model with accuracy: 51.86 %\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407d5aed"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the trained model's performance on a test set using appropriate metrics for binary classification, such as accuracy, precision, recall, and F1-score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1c532ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Evaluate the trained model's performance on a test set using appropriate metrics for multi-class classification and print the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca1b3465",
        "outputId": "46272c60-fc20-49ff-a5f9-0ca8b77316ef"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "# Load the saved best model state dictionary\n",
        "model = CNN_LSTM(input_size, hidden_size, num_layers, num_classes, sequence_length).to(device)\n",
        "model.load_state_dict(torch.load('best_cnn_lstm_model.pth'))\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Initialize lists to store true labels and predicted labels\n",
        "true_labels = []\n",
        "predicted_labels = []\n",
        "\n",
        "# Iterate through the test data loader and make predictions\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        true_labels.extend(labels.cpu().numpy())\n",
        "        predicted_labels.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Calculate accuracy, precision, recall, and F1-score\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "precision, recall, f1_score, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='weighted')\n",
        "\n",
        "# Print the calculated evaluation metrics\n",
        "print(f'Test Accuracy: {accuracy:.4f}')\n",
        "print(f'Test Precision: {precision:.4f}')\n",
        "print(f'Test Recall: {recall:.4f}')\n",
        "print(f'Test F1-score: {f1_score:.4f}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.5186\n",
            "Test Precision: 0.5235\n",
            "Test Recall: 0.5186\n",
            "Test F1-score: 0.5071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "691b8fde"
      },
      "source": [
        "## Pretraining\n",
        "\n",
        "### Subtask:\n",
        "Describe how this trained model can be used as a pretraining step for a larger, main model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8bf64ad"
      },
      "source": [
        "**Reasoning**:\n",
        "Explain the concept of pretraining with the CNN-LSTM model and how its learned weights can be transferred to a larger model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea3b15bf",
        "outputId": "ebd57b00-69b5-4eb0-843b-965fda1843c8"
      },
      "source": [
        "print(\"\"\"\n",
        "Using the trained CNN-LSTM model as a pretraining step for a larger, main model involves leveraging the knowledge learned by the CNN and/or LSTM layers on the synthetic stock pattern data. The idea is that these layers have learned to extract relevant features and temporal dependencies from the time series data, which can be beneficial for the main model, especially if the main model is trained on a more complex or larger dataset, or if the main dataset is limited.\n",
        "\n",
        "Here's how the weights could be transferred:\n",
        "\n",
        "1.  **Transferring CNN Weights:** The weights from the trained `conv1d` and potentially the `relu` and `maxpool1d` layers of the current `CNN_LSTM` model can be directly copied to the corresponding layers in the larger, main model. The main model would need to have a CNN component with a similar architecture (e.g., same number of filters, kernel size) for this transfer to be straightforward.\n",
        "\n",
        "2.  **Transferring LSTM Weights:** Similarly, the weights from the trained `lstm` layers (including the recurrent weights and biases) can be transferred to the LSTM layers in the larger, main model. Again, the main model's LSTM component should have a compatible architecture (e.g., same hidden size, number of layers).\n",
        "\n",
        "3.  **Transferring Both CNN and LSTM Weights:** A common approach is to transfer the weights from both the CNN and LSTM parts of the pretrained model to the main model. This allows the main model to benefit from both the learned spatial (feature extraction) and temporal (sequence modeling) representations.\n",
        "\n",
        "The potential benefits of using this pretraining approach for the main model are significant:\n",
        "\n",
        "*   **Improved Feature Extraction:** The pretrained CNN layers can provide a good starting point for extracting relevant features from the stock data, which might be more robust than training from scratch, especially if the main dataset is small.\n",
        "*   **Better Temporal Modeling:** The pretrained LSTM layers can help the main model better capture the temporal dynamics and dependencies in the stock market time series.\n",
        "*   **Faster Convergence:** Starting with pretrained weights can often lead to faster convergence during the training of the main model, as the model doesn't have to learn basic feature extraction and temporal patterns from scratch.\n",
        "*   **Reduced Overfitting (potentially):** Pretraining on a related task or dataset can act as a form of regularization, potentially reducing overfitting on the main task, especially if the main dataset is limited. This is particularly relevant given the overfitting observed in the current model training.\n",
        "\n",
        "Considerations and strategies when using pretraining include:\n",
        "\n",
        "*   **Freezing Layers:** Initially, you might want to freeze the transferred layers (CNN and/or LSTM) in the main model and train only the new layers (e.g., the final classification layers). This allows the main model to adapt to the pretrained features without drastically changing them.\n",
        "*   **Fine-tuning:** After initial training with frozen layers, you can unfreeze some or all of the transferred layers and continue training with a smaller learning rate. This allows the pretrained weights to be fine-tuned to the specific characteristics of the main dataset and task.\n",
        "*   **Architecture Compatibility:** Ensure the architecture of the transferred layers in the main model is compatible with the pretrained model's layers for smooth weight transfer.\n",
        "*   **Domain Relevance:** The effectiveness of pretraining depends on the relevance of the pretraining task and dataset to the main task. In this case, pretraining on synthetic stock patterns is relevant to predicting stock market movement.\n",
        "\"\"\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using the trained CNN-LSTM model as a pretraining step for a larger, main model involves leveraging the knowledge learned by the CNN and/or LSTM layers on the synthetic stock pattern data. The idea is that these layers have learned to extract relevant features and temporal dependencies from the time series data, which can be beneficial for the main model, especially if the main model is trained on a more complex or larger dataset, or if the main dataset is limited.\n",
            "\n",
            "Here's how the weights could be transferred:\n",
            "\n",
            "1.  **Transferring CNN Weights:** The weights from the trained `conv1d` and potentially the `relu` and `maxpool1d` layers of the current `CNN_LSTM` model can be directly copied to the corresponding layers in the larger, main model. The main model would need to have a CNN component with a similar architecture (e.g., same number of filters, kernel size) for this transfer to be straightforward.\n",
            "\n",
            "2.  **Transferring LSTM Weights:** Similarly, the weights from the trained `lstm` layers (including the recurrent weights and biases) can be transferred to the LSTM layers in the larger, main model. Again, the main model's LSTM component should have a compatible architecture (e.g., same hidden size, number of layers).\n",
            "\n",
            "3.  **Transferring Both CNN and LSTM Weights:** A common approach is to transfer the weights from both the CNN and LSTM parts of the pretrained model to the main model. This allows the main model to benefit from both the learned spatial (feature extraction) and temporal (sequence modeling) representations.\n",
            "\n",
            "The potential benefits of using this pretraining approach for the main model are significant:\n",
            "\n",
            "*   **Improved Feature Extraction:** The pretrained CNN layers can provide a good starting point for extracting relevant features from the stock data, which might be more robust than training from scratch, especially if the main dataset is small.\n",
            "*   **Better Temporal Modeling:** The pretrained LSTM layers can help the main model better capture the temporal dynamics and dependencies in the stock market time series.\n",
            "*   **Faster Convergence:** Starting with pretrained weights can often lead to faster convergence during the training of the main model, as the model doesn't have to learn basic feature extraction and temporal patterns from scratch.\n",
            "*   **Reduced Overfitting (potentially):** Pretraining on a related task or dataset can act as a form of regularization, potentially reducing overfitting on the main task, especially if the main dataset is limited. This is particularly relevant given the overfitting observed in the current model training.\n",
            "\n",
            "Considerations and strategies when using pretraining include:\n",
            "\n",
            "*   **Freezing Layers:** Initially, you might want to freeze the transferred layers (CNN and/or LSTM) in the main model and train only the new layers (e.g., the final classification layers). This allows the main model to adapt to the pretrained features without drastically changing them.\n",
            "*   **Fine-tuning:** After initial training with frozen layers, you can unfreeze some or all of the transferred layers and continue training with a smaller learning rate. This allows the pretrained weights to be fine-tuned to the specific characteristics of the main dataset and task.\n",
            "*   **Architecture Compatibility:** Ensure the architecture of the transferred layers in the main model is compatible with the pretrained model's layers for smooth weight transfer.\n",
            "*   **Domain Relevance:** The effectiveness of pretraining depends on the relevance of the pretraining task and dataset to the main task. In this case, pretraining on synthetic stock patterns is relevant to predicting stock market movement.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "694fbc75"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The CNN-LSTM model was successfully implemented in PyTorch, combining convolutional layers for feature extraction and LSTM layers for sequential dependencies. The output layer is designed for multi-class classification, not binary as initially stated in the task description.\n",
        "*   The data was successfully loaded, processed, and reshaped into PyTorch tensors with the shape `(number of samples, sequence length, number of features)`, which is `(2800, 60, 5)` for the training set and `(700, 60, 5)` for the testing set.\n",
        "*   The target variable was converted to numerical format, and the number of unique classes was identified as 7, not 2 as implied by \"binary classification (up or down)\" in the task.\n",
        "*   The training loop was implemented, and the model was trained for 10 epochs. The training loss decreased, and the accuracy on the test set improved to 51.86%, which is significantly better than the initially reported 16% but still indicates poor performance and potential issues.\n",
        "*   Evaluation on the test set using accuracy, precision, recall, and F1-score confirmed the model's poor performance, with all metrics around 0.52 or lower, suggesting the model is not effectively learning to predict the stock movements.\n",
        "*   The concept of using the trained model for pretraining was explained, detailing how CNN and LSTM weights could be transferred to a larger model and the potential benefits such as improved feature extraction, better temporal modeling, faster convergence, and potentially reduced overfitting.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The discrepancy between the task description (binary classification) and the actual data (7 classes) needs to be addressed. Either the data should be filtered for binary outcomes, or the model and evaluation should be adjusted for multi-class classification.\n",
        "*   The model's poor performance (accuracy ~52%) and the initial report of overfitting suggest that the model architecture, hyperparameters, or training strategy need significant adjustments. Techniques like regularization (dropout, weight decay), hyperparameter tuning, or exploring different model architectures could be beneficial.\n"
      ]
    }
  ]
}